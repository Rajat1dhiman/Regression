{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "\n",
        "Ans. Simple Linear Regression is a basic statistical method used to model the relationship between two variables:\n",
        "\n",
        "* One **independant variable** (input, or predictor), usually called X.\n",
        "* One **dependant variable** (output, or response), usually called Y.\n",
        "\n",
        "The goal is to find the best-fitting **Straight line** through the data points that predicts Y based on X."
      ],
      "metadata": {
        "id": "LZ3ehMw-wqoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Ans.**Key Assumptions of Simple Linear Regression**\n",
        "\n",
        "1. **Linearity**\n",
        "\n",
        "* The Relationship between the independant variable and dependant variable is **linear**.\n",
        "\n",
        "* This means the change in Y is proportional to the change in X\n",
        "\n",
        "2. **Independance of Errors**\n",
        "\n",
        "* The residuals are independant.\n",
        "\n",
        "* Especially important in **time series data**- there should be no autocorrelation.\n",
        "\n",
        "3. **Constant Variance of Errors**\n",
        "\n",
        "* The variance of the residuals is constant across all levels of X.\n",
        "\n",
        "* If the spread of residuals increases or decrease with X, this assumption is violated.\n",
        "\n",
        "4. **Normality of Residuals**\n",
        "\n",
        "* The residuals should be **approximately normally distributed**\n",
        "\n",
        "* This is especially important for confidence intervals and hypothesis testing.\n"
      ],
      "metadata": {
        "id": "Lmg-ZmMjMAST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "Ans. **What Does m (the Slope) Represent?**\n",
        "\n",
        "* Change in Y for every one-unit change in X\n",
        "\n",
        "In simple terms, how steep the line is, and in which direction it's going\n",
        "\n",
        "**Interpretation**\n",
        "\n",
        "* If m > 0: There's a positive relationship between X and Y\n",
        "→ As X increases, Y increases\n",
        "\n",
        "* If m < 0: There's a negative relationship\n",
        "→ As X increases, Y decreases\n",
        "\n",
        "* If m = 0: There's no relationship\n",
        "→ Y doesn’t change when X changes (flat line)\n",
        "\n"
      ],
      "metadata": {
        "id": "qrOGzfrlNw99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c ?\n",
        "\n",
        "Ans.\n",
        "* c is the intercept (also called the Y-intercept).\n",
        "\n",
        "* It represents the value of Y when X = 0.\n",
        "\n",
        "In other words, it's the point where the line crosses the Y-axis on a graph.\n",
        "\n",
        "**Interpretation**\n",
        "\n",
        "* Think of c as the starting value of Y before any change in X happens.\n",
        "\n",
        "* It sets the baseline of the relationship between X and Y."
      ],
      "metadata": {
        "id": "RzBwcS0aOTVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression ?\n",
        "\n",
        "Ans. **Formula to Calculate the Slope m:**\n",
        "\n",
        "𝑚\n",
        "=\n",
        "𝑛\n",
        "∑\n",
        "𝑋\n",
        "𝑌\n",
        "−\n",
        "∑\n",
        "𝑋\n",
        "∑\n",
        "𝑌\n",
        "𝑛\n",
        "∑\n",
        "𝑋\n",
        "2\n",
        "−\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        ")\n",
        "2\n",
        "m=\n",
        "n∑X\n",
        "2\n",
        " −(∑X)\n",
        "2\n",
        "\n",
        "n∑XY−∑X∑Y\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑛\n",
        "n = number of data points\n",
        "\n",
        "∑\n",
        "𝑋\n",
        "𝑌\n",
        "∑XY = sum of the product of corresponding X and Y values\n",
        "\n",
        "∑\n",
        "𝑋\n",
        "∑X = sum of all X values\n",
        "\n",
        "∑\n",
        "𝑌\n",
        "∑Y = sum of all Y values\n",
        "\n",
        "∑\n",
        "𝑋\n",
        "2\n",
        "∑X\n",
        "2\n",
        "  = sum of the squares of X values\n",
        "\n",
        "**Step-by-Step (Simplified Version)**\n",
        "\n",
        "If you have a small dataset, here's what you do:\n",
        "\n",
        "1. Compute the means of X and Y:\n",
        "\n",
        "𝑋\n",
        "ˉ\n",
        ",\n",
        "\n",
        "𝑌\n",
        "ˉ\n",
        "X\n",
        "ˉ\n",
        " ,\n",
        "Y\n",
        "ˉ\n",
        "\n",
        "2. Use the alternative (easier) formula:\n",
        "\n",
        "𝑚\n",
        "=\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "(\n",
        "𝑌\n",
        "−\n",
        "𝑌\n",
        "ˉ\n",
        ")\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "m=\n",
        "∑(X−\n",
        "X\n",
        "ˉ\n",
        " )\n",
        "2\n",
        "\n",
        "∑(X−\n",
        "X\n",
        "ˉ\n",
        " )(Y−\n",
        "Y\n",
        "ˉ\n",
        " )\n",
        "​\n",
        "\n",
        "Which is the:\n",
        "\n",
        "* Covariance of X and Y divided by the Variance of X"
      ],
      "metadata": {
        "id": "wk4n688sOtVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Ans. The purpose of the **least squares method in Simple linear Regression** is to find the best- fitting straight line through a set of data points by minimizing the sum of the squares of the vertical distances(errors or residuals) between the observed values and the values predicted by line.\n",
        "\n",
        "**In simple terms:**\n",
        "\n",
        "* It finds the line that makes the total error between actual and predicted values as small as possible."
      ],
      "metadata": {
        "id": "jtjXyfo7PMPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "Ans. In Simple Linear Regression, the coefficient of determination (R²) tells you how well the regression line explains the variation in the dependent variable (Y) based on the independent variable (X).\n",
        "\n",
        "**Interpretation of R²:**\n",
        "\n",
        "* R² = 1: The regression line perfectly fits the data; 100% of the variation in Y is explained by X.\n",
        "\n",
        "* R² = 0: The regression line explains none of the variation in Y.\n",
        "\n",
        "* Between 0 and 1: Represents the proportion of the variance in Y that can be explained by X.\n",
        "For example, **R² = 0.85 means 85% of the variability in Y** is explained by X.\n",
        "\n",
        "Higher R² generally indicates a better fit—but it doesn’t mean the model is correct or useful on its own."
      ],
      "metadata": {
        "id": "WCLPGD4gXdp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "Ans. Multiple Linear Regression is an extension of simple linear regression that models the relationship between a dependent variable (Y) and two or more independent variables (X₁, X₂, ..., Xₙ).\n",
        "\n",
        "The general equation is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜀\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +...+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " +ε"
      ],
      "metadata": {
        "id": "maBtv_CRX5cO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans. The main difference between Simple and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable:\n",
        "\n",
        "* **Simple Linear Regression:**\n",
        "\n",
        "* Uses **one independent variable** to predict the dependent variable.\n",
        "\n",
        "➤ Equation:\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝜀\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+ε\n",
        "\n",
        "* **Multiple Linear Regression:**\n",
        "\n",
        "Uses **two or more independent variables** to predict the dependent variable.\n",
        "\n",
        "➤ Equation:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜀\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +...+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " +ε\n",
        "\n",
        "In short:\n",
        "\n",
        "**Simple = one predictor,**\n",
        "**Multiple = many predictors.**"
      ],
      "metadata": {
        "id": "2yb5kpgzYO4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression ?\n",
        "\n",
        "Ans. **Key Assumptions**\n",
        "\n",
        "1. **Linearity**: The relationship between the dependant variable and each independant variable is linear.\n",
        "\n",
        "2. **Independence of Errors**: The errors are independant ; no autocorrelation.\n",
        "\n",
        "3. **Normality of Errors**: The residuals are normally disputed(especially important for confidence intervals and hypothesis testing).\n",
        "\n",
        "4. **No Multicollinearity**: The independant variables are not highly correlated with each other.\n",
        "\n",
        "5. **Homoscedasticity**: The residuals have constant variance at all levels of the independant variables.\n"
      ],
      "metadata": {
        "id": "g894A5mHY9qF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Ans. **Heteroscedasticity** occurs when the **variance or errors** in  a regression model is **not constant** across all levels of the independant variables.\n",
        "\n",
        "**In simple Terms**\n",
        "\n",
        "Instead of the residuals spreading out evenly, they fan out or shrink as the predicted values increase or decrease.\n",
        "\n",
        "**Why it's a problem in Multiple Linear Regression:**\n",
        "\n",
        "* It violates the assumption of homoscedasticity, which is required for the standard errors to be reliable.\n",
        "\n",
        "* As a result, your confidence intervals and hypothesis tests (like t-tests on coefficients) may become invalid.\n",
        "\n",
        "* You might incorrectly conclude that a variable is significant when it's not (or vice versa).\n"
      ],
      "metadata": {
        "id": "GXLUGJOTaRMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Ans. If your **Multiple Linear Regression model has high multicollinearity** (when independent variables are highly correlated), it can lead to unstable coefficient estimates and make it hard to interpret the effect of each variable.\n",
        "\n",
        "**Ways to improve your model:**\n",
        "\n",
        "1. Remove one of the correlated variables\n",
        "\n",
        "* If two predictors are giving redundant information, drop one.\n",
        "\n",
        "2. Combine correlated variables\n",
        "\n",
        "* Use techniques like Principal Component Analysis (PCA) or create a composite score.\n",
        "\n",
        "3. Use regularization techniques\n",
        "\n",
        "* Apply Ridge Regression (L2) or Lasso Regression (L1), which can reduce coefficient size or even eliminate some variables.\n",
        "\n",
        "4. Check Variance Inflation Factor (VIF)\n",
        "\n",
        "* Drop or adjust variables with VIF > 5 or 10, indicating strong multicollinearity.\n",
        "\n",
        "5. Collect more data\n",
        "\n",
        "* Sometimes more observations can reduce the sensitivity to multicollinearity."
      ],
      "metadata": {
        "id": "uB8k4c8fbERY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Ans. **Common techniques to transform categorical variables:**\n",
        "\n",
        "1. **One-Hot Encoding (Dummy Variables)**\n",
        "\n",
        "* Creates a new binary column for each category (1 if it’s that category, 0 otherwise).\n",
        "* Best for nominal (unordered) categories.\n",
        "* Example: Color = [Red, Blue, Green] → Red: [1 0 0], Blue: [0 1 0], etc.\n",
        "\n",
        "2. **Label Encoding**\n",
        "\n",
        "*  Assigns a unique integer to each category.\n",
        "* Use only for ordinal data (like Low, Medium, High), not nominal.\n",
        "* Example: Size = [Small, Medium, Large] → [0, 1, 2].\n",
        "\n",
        "3. **Binary Encoding**\n",
        "\n",
        "* Combines label encoding and binary conversion, reducing dimensionality.\n",
        "* Useful when you have many categories.\n",
        "\n",
        "4. **Target Encoding (Mean Encoding)**\n",
        "\n",
        "* Replaces categories with the mean of the target variable for each category.\n",
        "* Can lead to data leakage — use carefully with cross-validation.\n",
        "\n",
        "5. **Frequency Encoding**\n",
        "\n",
        "* Encodes each category with how often it appears in the dataset."
      ],
      "metadata": {
        "id": "UCcIK4P-buoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Ans. An interaction term helps model **non-additive relationships** between variables. Without it, the model assumes that the predictors influence the response **independently** of each other.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Say you're modeling salary based on education level and years of experience.\n",
        "\n",
        "* Without interaction:\n",
        "  Salary = b0 + b1*Education + b2*Experience\n",
        "\n",
        "* With interaction:\n",
        "  Salary = b0 + b1*Education + b2*Experience + b3*(Education * Experience)\n",
        "\n",
        "That last term (Education * Experience) allows the model to say:\n",
        "\"The effect of experience on salary is different depending on your education level.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "ZXd8ew4MY2sE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression ?\n",
        "\n",
        "Ans. **In Simple Linear Regression (SLR):**\n",
        "\n",
        "You have one predictor:\n",
        "Y = b0 + b1*X\n",
        "\n",
        "* The **intercept (b0)** is the expected value of **Y when X = 0.**\n",
        "\n",
        "* It's often interpretable (if X = 0 makes sense in context).\n",
        "\n",
        "E.g., if you're modeling house price based on square footage, the intercept is the predicted price when size = 0 — maybe meaningless, but still mathematically clear.\n",
        "\n",
        "**In Multiple Linear Regression (MLR):**\n",
        "\n",
        "You have multiple predictors:\n",
        "Y = b0 + b1*X1 + b2*X2 + ... + bn*Xn\n",
        "\n",
        "* The **intercept (b0)** is the expected value of **Y when all Xs = 0.**\n",
        "\n",
        "* That’s **more abstract** — sometimes, none of the predictors being zero is realistic (e.g., age = 0, income = 0, etc.).\n",
        "\n",
        "* So the intercept is still technically the \"baseline\" value of Y, but only when all predictors are 0, which might not have real-world meaning.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CtzEx85rqeoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  What is the significance of the slope in regression analysis, and how does it affect predictions ?\n",
        "\n",
        "Ans. In regression (simple or multiple), the **slope** represents the **change in the response variable (Y)** for a one-unit change in a predictor (X), holding other variables constant (in multiple regression).\n",
        "\n",
        "\n",
        "**Significance of the slope:**\n",
        "\n",
        "1. **Direction of the relationship**\n",
        "\n",
        "* Positive slope → as X increases, Y increases.\n",
        "\n",
        "* Negative slope → as X increases, Y decreases.\n",
        "\n",
        "2. **Magnitude of change**\n",
        "\n",
        "* A slope of 3 means that for each 1 unit increase in X, Y increases by 3 units (on average).\n",
        "\n",
        "3. **Practical meaning**\n",
        "\n",
        "* In a salary model: if the slope for “years of experience” is 2,000, then each extra year adds $2,000 to predicted salary.\n",
        "\n",
        "**How does it affect predictions?**\n",
        "\n",
        "Slopes are used to calculate predicted Y values. If you change X, the slope tells you how much Y is expected to change.\n",
        "\n",
        "* Formula:\n",
        "Ŷ = b0 + b1*X1 + b2*X2 + ... + bn*Xn\n",
        "\n",
        "* Change one X?\n",
        "→ Prediction adjusts by slope × amount changed\n",
        "\n"
      ],
      "metadata": {
        "id": "_-Xp6PWvrHx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables ?\n",
        "\n",
        "Ans. In a regression model, the intercept (b₀) is the predicted value of the response variable (Y) when all predictor variables = 0.\n",
        "\n",
        "It acts as the baseline or starting point for predictions.\n",
        "\n",
        "**How it provides context:**\n",
        "\n",
        "1. **Baseline Understanding**\n",
        "\n",
        "* The intercept sets the stage:\n",
        "\n",
        "* “If none of the predictors are present, here’s where we begin.”\n",
        "\n",
        "* For example, in a model: Salary = 30,000 + 2,000*Experience + 5,000*DegreeLevel\n",
        "→ The $30,000 is the starting salary before experience or degree are considered.\n",
        "\n",
        "2. **Anchors the Relationship**\n",
        "\n",
        "* It gives a reference point from which the slopes operate.\n",
        "\n",
        "* So you can understand not just how much X affects Y, but from where that change begins.\n",
        "\n",
        "3. **Contextual Realism**\n",
        "\n",
        "* Sometimes, interpreting the intercept reveals model limitations.\n",
        "\n",
        "* E.g., if the intercept predicts a negative number of sales when advertising spend is $0 — that might hint at model misfit or need for transformation.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "LAZPlOmWsBcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance ?\n",
        "\n",
        "Ans. **Limitations of using R² alone:**\n",
        "\n",
        "1. **Doesn’t Indicate Causation**\n",
        "\n",
        "* A high R² doesn’t mean X causes Y — just that they’re correlated.\n",
        "\n",
        "2. **Can Be Misleading with More Predictors**\n",
        "\n",
        "* Adding more variables will never decrease R², even if the new predictors are useless.\n",
        "\n",
        "* This can make a bad model look better.\n",
        "\n",
        "3. **Doesn’t Show Model Accuracy**\n",
        "\n",
        "* A model can have a high R² but still make large prediction errors.\n",
        "\n",
        "* R² doesn't directly reflect how close predictions are to actual values.\n",
        "\n",
        "4. **Not Suitable for Nonlinear Models**\n",
        "\n",
        "* In nonlinear regression, R² can be hard to interpret or misleading.\n",
        "\n",
        "5. **Ignores Overfitting**\n",
        "\n",
        "* High R² might mean the model is overfitting — capturing noise instead of signal.\n",
        "\n"
      ],
      "metadata": {
        "id": "hGfX_4UNsmJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would  you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Ans.\n",
        "1. **Uncertainty:**\n",
        "\n",
        "* The coefficient is **not estimated precisely**. It could vary a lot from sample to sample.\n",
        "\n",
        "2. **Weak Evidence:**\n",
        "\n",
        "* A large SE means that, even if the coefficient looks big, it might **not be statistically different from zero **— check the t-value and p-value.\n",
        "\n",
        "3. **Instability in Model:**\n",
        "\n",
        "* This could suggest **multicollinearity**   (predictors are highly correlated), or that you don't have enough data.\n",
        "\n"
      ],
      "metadata": {
        "id": "osGU-PCTtPNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it ?\n",
        "\n",
        "Ans. It means the **variance of the residuals (errors) is not constant** across all levels of the predictors.\n",
        "\n",
        "* In a good regression model, residuals should have **equal spread** (homoscedasticity).\n",
        "\n",
        "* Heteroscedasticity violates one of the key assumptions of linear regression.\n",
        "\n",
        "**How to spot it in a residual plot**:\n",
        "\n",
        "Make a plot of residuals vs. predicted values (or vs. an independent variable).\n",
        "\n",
        "**Signs of heteroscedasticity:**\n",
        "\n",
        "* **Funnel shape** (residuals spread out as predictions increase or decrease)\n",
        "\n",
        "* **Fan shape or curved pattern**\n",
        "\n",
        "* Residuals get **more or less spread out** at different levels of prediction\n",
        "\n",
        "* It should look like a **random cloud**. If there's a pattern → potential heteroscedasticity.\n",
        "\n",
        "**Why it matters:**\n",
        "\n",
        "1. **Bias in standard errors**\n",
        "\n",
        "* Can lead to wrong p-values and confidence intervals, so your conclusions may be invalid.\n",
        "\n",
        "2. **Unreliable hypothesis tests**\n",
        "\n",
        "* You might incorrectly think a variable is significant (or not).\n",
        "\n",
        "3. **Poor predictions for some ranges**\n",
        "\n",
        "* The model may perform well at low values but poorly at high ones (or vice versa)\n",
        "\n",
        "**How to address it:**\n",
        "\n",
        "* **Transform the response variable**(e.g., log(Y), √Y)\n",
        "\n",
        "* **Use weighted least squares (WLS)**\n",
        "\n",
        "* **Robust standard errors** (e.g., White’s correction)"
      ],
      "metadata": {
        "id": "aUnht3lrt9ZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R² ?\n",
        "\n",
        "Ans. **High R² but Low Adjusted R² — what’s going on?**\n",
        "\n",
        "* **R²** always increases (or stays the same) when you add more predictors, even if they’re **useless.**\n",
        "\n",
        "* **Adjusted R²** corrects for this by **penalizing** for the number of predictors — it tells you whether new variables are actually improving the model.\n",
        "\n",
        "So if:\n",
        "\n",
        "* **R² is high** → the model appears to explain a lot of the variance in Y\n",
        "\n",
        "* **Adjusted R² is much lower** → some predictors probably aren’t helping and may be **noise**\n",
        "\n"
      ],
      "metadata": {
        "id": "Lwc-YeqZvRkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  Why is it important to scale variables in Multiple Linear Regression ?\n",
        "\n",
        "Ans.\n",
        "1. **Improves interpretability when comparing coefficients**\n",
        "\n",
        "* Raw coefficients are affected by the **scale of the variable** (e.g., income in dollars vs. age in years).\n",
        "\n",
        "* Without scaling, you can’t easily compare which predictor has a bigger effect on the outcome.\n",
        "\n",
        "2. **Helps with numerical stability**\n",
        "\n",
        "* When variables are on very different scales, the matrix calculations behind the scenes can get messy and less accurate (especially with lots of variables).\n",
        "\n",
        "* Scaling helps the algorithm converge more reliably.\n",
        "\n",
        "3. **Crucial when using regularization (like Ridge or Lasso)**\n",
        "\n",
        "* These models penalize large coefficients — and without scaling, variables with large values (like house price) may dominate the penalty unfairly.\n",
        "\n",
        "4. **Detecting multicollinearity**\n",
        "\n",
        "* Variance Inflation Factor (VIF) can be affected by unscaled data; scaling helps get clearer diagnostics.\n",
        "\n"
      ],
      "metadata": {
        "id": "x3uYHUz-v3Te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "\n",
        "Ans. Polynomial regression is a type of regression where the **relationship between the independent variable X and the dependent variable Y is modeled as an nth-degree polynomial:**\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜀\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ε\n",
        "\n",
        "So instead of just fitting a straight line (like in linear regression), you're fitting a **curve.**\n",
        "\n"
      ],
      "metadata": {
        "id": "3NwPnO7vwcz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "Ans. **Linear Regression:**\n",
        "\n",
        "* **Assumption:** The relationship between the independent variable(s) (X) and the dependent variable (Y) is **linear.**\n",
        "\n",
        "* **Equation:**\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝜀\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+ε\n",
        "This means Y is predicted based on a straight line.\n",
        "\n",
        "* **Prediction:** The model tries to fit a straight line through the data, and the relationship between X and Y remains constant throughout (the slope is fixed).\n",
        "\n",
        "* **Use case:** Ideal when the relationship between the variables is expected to be linear (like predicting house prices from square footage).\n",
        "\n",
        "**Polynomial Regression:**\n",
        "\n",
        "* **Assumption:** The relationship between the independent variable(s) and the dependent variable is nonlinear, but can be modeled by a polynomial (quadratic, cubic, etc.).\n",
        "\n",
        "* **Equation:**\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝜀\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+ε\n",
        "This means that the model uses higher-degree terms (like\n",
        "𝑋\n",
        "2\n",
        "X\n",
        "2\n",
        " ,\n",
        "𝑋\n",
        "3\n",
        "X\n",
        "3\n",
        " , etc.) to create a curve.\n",
        "\n",
        "* **Prediction:** The model fits a curved line through the data, allowing for bends and turns (e.g., U-shapes, inverted U-shapes, etc.).\n",
        "\n",
        "* **Use case:** Useful when you believe the relationship between X and Y is more complex or has curvature (like predicting performance based on study hours, where too much study might hurt scores)."
      ],
      "metadata": {
        "id": "rIjP_77bwykb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "\n",
        "Ans. Polynomial regression is used in scenarios where the relationship between the independent variable(s) (predictors) and the dependent variable (outcome) is **nonlinear,** but can still be captured by a polynomial (such as a quadratic, cubic, etc.).\n",
        "\n",
        "Here are a few situations when polynomial regression is particularly useful:\n",
        "\n",
        "1. **When the relationship between variables is curvilinear:**\n",
        "\n",
        "* **Example**: Predicting **product sales based on advertising spend.** There might be a threshold effect where a small amount of advertising increases sales significantly, but after a certain point, more spending yields diminishing returns, creating a U-shape.\n",
        "\n",
        "* In this case, a linear model wouldn’t capture the curve, but a polynomial regression (like a quadratic model) would.\n",
        "\n",
        "2. **When the data shows a U-shaped or inverted U-shape pattern:**\n",
        "\n",
        "* **Example:** Modeling student performance based on study hours. Too few study hours and too many study hours might both result in low performance, but a moderate amount of study time leads to high performance.\n",
        "\n",
        "3. **When you suspect interaction effects or non-linear trends between variables:**\n",
        "\n",
        "* **Example:** Modeling housing prices based on size of the house and age of the house. The effect of house size on price might depend on the age of the house (e.g., larger houses may become more expensive as they age, but only up to a certain point). A polynomial regression can capture such non-linear effects by incorporating interaction terms or higher-degree polynomials.\n",
        "\n",
        "4. **When you want to improve a model that is too simple (but still want to stick with regression):**"
      ],
      "metadata": {
        "id": "d9pA7u7axmIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        "Ans. The general equation for **polynomial regression** is an extension of the linear regression equation, but with higher-degree terms of the independent variable(s).\n",
        "\n",
        "**General Equation for Polynomial Regression (with one predictor):**\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜀\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ε\n",
        "\n",
        "Where:\n",
        "\n",
        "* 𝑌\n",
        "Y = dependent variable (response)\n",
        "\n",
        "* 𝑋\n",
        "X = independent variable (predictor)\n",
        "\n",
        "* 𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  = intercept (constant)\n",
        "\n",
        "* 𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        "  = coefficients for each term in the polynomial\n",
        "\n",
        "* 𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " ,…,X\n",
        "n\n",
        "  = higher-order terms (squared, cubed, etc.)\n",
        "\n",
        "𝜀\n",
        "* ε = error term (captures unexplained variance)"
      ],
      "metadata": {
        "id": "6IBWMuqPyiLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression  be applied to multiple variables?\n",
        "\n",
        "Ans. Yes, **polynomial regression** can be applied to multiple variables, and this is often called **multivariable polynomial regression or multivariate polynomial regression**. In this case, the relationship between the dependent variable (\n",
        "𝑌\n",
        "Y) and multiple independent variables (\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑝\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "p\n",
        "​\n",
        " ) is modeled with polynomial terms.\n",
        "\n",
        "**General Form of Multivariable Polynomial Regression:**\n",
        "\n",
        "When you have multiple predictors, the polynomial regression equation becomes more complex, as you include **interaction terms and higher powers** of each predictor. Here's the general form:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "1\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "4\n",
        "𝑋\n",
        "2\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "5\n",
        "𝑋\n",
        "1\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝜀\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "1\n",
        "2\n",
        "​\n",
        " +β\n",
        "4\n",
        "​\n",
        " X\n",
        "2\n",
        "2\n",
        "​\n",
        " +β\n",
        "5\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+ε"
      ],
      "metadata": {
        "id": "O5nqhnGLzM7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "Ans. Polynomial regression can be a powerful tool, but it does come with several limitations that you should be aware of. Here are the key drawbacks:\n",
        "\n",
        "1. **Risk of Overfitting**\n",
        "\n",
        "* **Problem:** Polynomial regression models can easily overfit the data, especially if you use higher-degree polynomials (e.g., cubic, quartic). The model will become overly complex, fitting not only the underlying pattern but also the noise in the data.\n",
        "\n",
        "* **Consequences:** Overfitting leads to poor generalization to new, unseen data. Your model might perform great on the training data but poorly on validation or test sets.\n",
        "\n",
        "* **Solution:** Use cross-validation to check if the model is generalizing well. Also, consider simplifying the model or using regularization (e.g., Ridge or Lasso regression).\n",
        "\n",
        "2. **Complexity and Interpretability**\n",
        "\n",
        "* **Problem:** As you increase the degree of the polynomial or add multiple interaction terms, the model becomes difficult to interpret. You might have trouble understanding how the individual predictors (e.g.,\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ) are influencing the dependent variable (\n",
        "𝑌\n",
        "Y).\n",
        "\n",
        "* **Consequences:** High-degree polynomials can make it harder to explain the model to non-technical stakeholders, especially when interaction terms and higher-order terms are involved.\n",
        "\n",
        "* **Solution:** Keep the degree of the polynomial low (e.g., quadratic or cubic) to maintain some interpretability, or use alternative models that are easier to interpret (e.g., decision trees).\n",
        "\n",
        "3. **Overemphasis on Outliers**\n",
        "\n",
        "* **Problem:** High-degree polynomials are sensitive to outliers in the data. A few extreme values can cause the curve to bend dramatically to accommodate them, leading to a poor fit.\n",
        "\n",
        "* **Consequences:** The model can produce unrealistic predictions, especially in regions where there are few data points but extreme values.\n",
        "\n",
        "* **Solution:** Remove or mitigate outliers before fitting the polynomial regression model. Alternatively, use robust regression methods that are less sensitive to outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "4V-APh5zz4ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial ?\n",
        "\n",
        "Ans. 1. Cross-Validation\n",
        "What it is: Cross-validation is a technique where the data is split into multiple folds (usually 5 or 10). The model is trained on some of the folds and tested on the remaining fold(s) to evaluate performance. This process is repeated multiple times to get an average performance score.\n",
        "\n",
        "Why it’s useful: Cross-validation helps assess how well the model generalizes to unseen data, making it less prone to overfitting. It provides a robust evaluation of the model's performance by evaluating it on different subsets of the data.\n",
        "\n",
        "How to use it:\n",
        "\n",
        "Train the polynomial regression model on different degrees (e.g., 1st, 2nd, 3rd, etc.).\n",
        "\n",
        "Evaluate performance using cross-validation (e.g., k-fold cross-validation).\n",
        "\n",
        "Compare the mean squared error (MSE) or root mean squared error (RMSE) across different degrees. A lower cross-validation error suggests a better model.\n",
        "\n",
        "2. **Adjusted R-squared (Adjusted\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        " )**\n",
        "* What it is: The adjusted\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  is a modified version of the traditional\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  that penalizes the model for having too many predictors. It helps to avoid overfitting by adjusting for the number of features (or terms in the polynomial) in the model.\n",
        "\n",
        "* Why it’s useful: As you increase the degree of the polynomial, the regular\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  will always increase (or stay the same), even if the additional complexity is unnecessary. Adjusted\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  accounts for the number of predictors and will only increase if the additional terms significantly improve the model.\n",
        "\n",
        "**How to use it:**\n",
        "\n",
        "* Compute\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  and adjusted\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  for each degree of the polynomial.\n",
        "\n",
        "* Adjusted\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  will help you avoid selecting a high-degree polynomial that only increases\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  due to unnecessary complexity.\n",
        "\n",
        "3. **Akaike Information Criterion (AIC) / Bayesian Information Criterion (BIC)**\n",
        "\n",
        "* What they are: AIC and BIC are model selection criteria that penalize the likelihood of the model for having more parameters (complexity). These criteria help balance the goodness of fit with model complexity. Lower values indicate a better fit."
      ],
      "metadata": {
        "id": "vqN_w1UD0obj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        "Ans. 1. **Understanding the Relationship Between Variables**\n",
        "\n",
        "* Polynomial regression is used when the relationship between the dependent and independent variable is nonlinear. Visualizing the data and the fitted polynomial curve allows you to better understand the nature of this relationship.\n",
        "\n",
        "* For example, with a quadratic model (degree 2), you might observe a parabolic shape indicating a U-curve or an inverted U-curve.\n",
        "\n",
        "* Why it matters: Visualizing helps to confirm whether the polynomial regression is indeed capturing the intended curved relationship, rather than a simple linear one.\n",
        "\n",
        "2. **Assessing Model Fit**\n",
        "\n",
        "* Visualization of the polynomial curve fitted to the data can give you an immediate sense of how well the model represents the data. For example:\n",
        "\n",
        "* In a 1D regression, you can plot the original data points against the fitted curve to see if it captures the overall trend without overfitting or underfitting.\n",
        "\n",
        "* In higher dimensions, you can visualize using contour plots or 3D plots to understand how the polynomial terms interact.\n",
        "\n",
        "* Why it matters: A good fit should follow the data trend while not being overly complex. If the curve oscillates too much or becomes too jagged, it may indicate overfitting.\n",
        "\n",
        "3. **Detecting Overfitting**\n",
        "\n",
        "* **Polynomial regression models** are prone to overfitting, especially when the polynomial degree is too high. Visualizing the polynomial curve allows you to see if the model is fitting the noise rather than the underlying pattern in the data.\n",
        "\n",
        "Why it matters:\n",
        "\n",
        "* For example, a **high-degree polynomial** may fit the training data perfectly but produce a very wavy curve that doesn't generalize well to unseen data.\n",
        "\n",
        "* Overfitting is often visually evident, as the curve may \"wiggle\" excessively in areas where there is no real data trend.\n",
        "\n",
        "* **How visualization helps:** By plotting the data alongside the fitted model, you can spot overfitting when the curve appears too complex or changes direction sharply.\n",
        "\n"
      ],
      "metadata": {
        "id": "rekB5Xon1Mi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in python?\n",
        "\n",
        "Ans. **Steps for Implementing Polynomial Regression in Python**\n",
        "\n",
        "1. **Import Necessary Libraries**\n",
        "\n",
        "* You’ll need libraries like **NumPy, matplotlib, and scikit-learn** for polynomial regression.\n",
        "\n",
        "2. **Prepare the Data**\n",
        "\n",
        "* You’ll need to have a dataset with an independent variable\n",
        "𝑋\n",
        "X and a dependent variable\n",
        "𝑌\n",
        "Y.\n",
        "\n",
        "3. **Create Polynomial Features**\n",
        "\n",
        "* Polynomial regression is implemented by transforming the features into polynomial terms using PolynomialFeatures from scikit-learn.\n",
        "\n",
        "4. **Fit the Polynomial Regression Model**\n",
        "\n",
        "Use LinearRegression from scikit-learn to fit the transformed features.\n",
        "\n",
        "5. **Visualize the Results**\n",
        "\n",
        "* Use matplotlib to plot the original data points and the fitted polynomial curve."
      ],
      "metadata": {
        "id": "KOAhWqba148H"
      }
    }
  ]
}